
## 3 vs of Data 
  
The "3 Vs of data" refer to three key characteristics that are often used to describe and differentiate various aspects of data. These three Vs are:

1. **Volume:**
    
    - **Definition:** Refers to the sheer size or quantity of data.
    - **Example:** The amount of data generated by sensors, social media, transactions, etc.
    - **Challenge:** Dealing with the scale of data and ensuring effective storage and processing capabilities.
2. **Velocity:**
    
    - **Definition:** Refers to the speed at which data is generated, processed, and made available.
    - **Example:** Real-time data streaming from sources like social media, sensors, and financial transactions.
    - **Challenge:** Handling and analyzing data in near real-time to derive meaningful insights.
3. **Variety:**
    
    - **Definition:** Refers to the different types and formats of data.
    - **Example:** Data can come in structured formats (like databases), semi-structured (like XML or JSON), or unstructured (like text documents, images, videos).
    - **Challenge:** Managing and integrating diverse data types to extract valuable information.

These three Vs are commonly associated with big data, a term used to describe datasets that are too large or complex for traditional data processing applications. As technology and data science continue to evolve, additional Vs have been proposed, such as Veracity (dealing with the reliability of data) and Value (focusing on extracting meaningful insights).

## ETL Pipeline 
An ETL (Extract, Transform, Load) pipeline is a set of processes and tools used to collect data from various sources, transform it into a desired format, and load it into a target data store, typically a data warehouse or database. The primary goal of an ETL pipeline is to enable efficient and effective data integration, ensuring that data is available in a consistent and usable format for analysis, reporting, and other business intelligence purposes. Let's break down each phase of the ETL process:

1. **Extract:**
    
    - _Definition:_ In the extraction phase, data is gathered from multiple sources. These sources can include databases, flat files, APIs, logs, and more.
    - _Process:_ Data is extracted in its raw form from source systems. This process involves querying databases, reading files, or pulling data from APIs.
    - _Challenges:_ Handling different data formats, dealing with various APIs, managing large volumes of data, and ensuring data consistency during extraction.
2. **Transform:**
    
    - _Definition:_ In the transformation phase, extracted data undergoes cleaning, restructuring, and enrichment to meet the requirements of the target system.
    - _Process:_ Data transformation involves tasks such as cleaning and validating data, aggregating or disaggregating information, converting data types, and applying business rules and logic.
    - _Challenges:_ Dealing with inconsistencies, resolving data quality issues, handling missing or incomplete data, and ensuring that transformed data aligns with the intended data model.
3. **Load:**
    
    - _Definition:_ In the loading phase, the transformed data is loaded into the target data store, such as a data warehouse or database.
    - _Process:_ Data is loaded into the destination system, and this can involve inserting, updating, or appending records as needed. Loading may be incremental or full, depending on the requirements.
    - _Challenges:_ Ensuring data integrity during loading, handling duplicate records, managing the volume of data, and optimizing the loading process for performance.

**ETL Pipeline Workflow:**

- **Initialization:** ETL processes are often scheduled to run at specific intervals, initiated manually, or triggered by events.
- **Extraction:** Data is extracted from source systems using various methods like direct queries, API calls, or file reading.
- **Transformation:** Extracted data undergoes cleansing, validation, and transformation based on predefined business rules.
- **Loading:** Transformed data is loaded into the target data store, ready for analysis and reporting.

**Key Components of ETL Pipeline:**

1. **Connectors:** Interface with source and destination systems.
2. **Data Transformation Logic:** Business rules and transformations applied to the data.
3. **Job Scheduler:** Controls the timing and execution of the ETL processes.
4. **Monitoring and Logging:** Tracks the status, performance, and issues during the ETL process.
5. **Metadata Repository:** Stores information about the data sources, transformations, and loading processes.

**ETL Tools:** There are numerous ETL tools available, both open source and commercial, that facilitate the development and management of ETL pipelines. Examples include Apache NiFi, Talend, Informatica, and Microsoft SSIS (SQL Server Integration Services). These tools often provide a visual interface for designing, testing, and deploying ETL workflows, making the process more accessible to users with varying levels of technical expertise.



## Distributed File System 

### Scalability 


### What is Commodity Hardware 
Commodity hardware refers to inexpensive, readily available, and standardized computing equipment that is widely used for general-purpose computing tasks. It is contrasted with specialized or proprietary hardware that is designed for specific applications or industries.

### Distributed System 
Distributed System is a collection of autonomous computer systems that are physically separated but are connected by a centralized computer network that is equipped with distributed system software. The autonomous computers will communicate among each system by sharing resources and files and performing the tasks assigned to them.

**Types of Distributed Systems:***

There are many models and architectures of distributed systems in use today.

- ***Client-server systems***, the most traditional and simple type of distributed system, involve a multitude of networked computers that interact with a central server for data storage, processing or other common goal.
- ***Peer-to-peer networks*** distribute workloads among hundreds or thousands of computers all running the same software.
- ***Cell phone networks*** are an advanced distributed system, sharing workloads among handsets, switching systems and internet-based devices.


### Hadoop File System
HDFS (Hadoop Distributed File System) is the primary storage system used by Hadoop applications. This open source framework works by rapidly transferring data between nodes. It's often used by companies who need to handle and store big data. HDFS is a key component of many Hadoop systems, as it provides a means for managing big data, as well as supporting big data analytics.

This is where Hadoop comes in. It provides one of the most reliable filesystems. HDFS (Hadoop Distributed File System) is a unique design that provides storage for extremely large files with streaming data access pattern and it runs on commodity hardware.

Let’s elaborate the terms:  
Extremely large files: Here we are talking about the data in range of petabytes(1000 TB).
Streaming Data Access Pattern: HDFS is designed on principle of write-once and read-many-times. Once data is written large portions of dataset can be processed any number times.
Commodity hardware: Hardware that is inexpensive and easily available in the market. This is one of feature which specially distinguishes HDFS from other file system.

Nodes: Master-slave nodes typically forms the HDFS cluster. 

![[Pasted image 20240224155608.png]]

#### Namenode

The NameNode is the master node of HDFS and stores the metadata and slave configuration. In HDFS, there is one active NameNode and one or more standby NameNodes. The Active NameNode serves all client requests, and the standby NameNode handles high availability configuration.

**Functions of NameNode:**

- It manages the File system namespace and is the single Hadoop cluster failure point.
- It keeps track of all blocks in HDFS and where each block is located.
    
- It manages the client access requests for the actual data files.
- The metadata about the actual data is also stored here, like File information, Block information, permissions, etc.

#### DataNode

DataNode is a worker node of HDFS, which can be n in number. This node is responsible for serving read and write requests to clients. Datanodes store actual data in HDFS, so they typically have a lot of hard disk space.

**Functions of DataNode:**

- It stores actual data in HDFS.
- As instructed by the NameNode, the DataNodes are responsible for storing and deleting blocks and replicating these blocks.
- This node handles the client’s read and writes requests.
- DataNodes are synchronized to communicate and ensure that data is balanced across the cluster, moving data for high replication, and copying data as needed.

![[Pasted image 20240224155117.png]]

### Working of Hadoop File System
![[Pasted image 20240224155345.png]]

Lets assume that 100TB file is inserted, then masternode(namenode) will first _divide_ the file into blocks of 10TB (default size is _128 MB_ in Hadoop 2.x and above). Then these blocks are stored across different datanodes(slavenode). 
Datanodes(slavenode)_replicate_ the blocks among themselves and the information of what blocks they contain is sent to the master. Default replication factor is _3_ means for each block 3 replicas are created (including itself). In hdfs.site.xml we can increase or decrease the replication factor i.e we can edit its configuration here.

**Why divide the file into blocks?**   
  
_Answer:_ Let’s assume that we don’t divide, now it’s very difficult to store a 100 TB file on a single machine. Even if we store, then each read and write operation on that _whole file_ is going to take very high seek time. But if we have multiple blocks of size 128MB then its become easy to perform various read and write operations on it compared to doing it on a whole file at once. So we divide the file to have faster data access i.e. reduce seek time.   
  
  
**Why replicate the blocks in data nodes while storing?**   
  
Data replication is crucial because it ensures data remains available even if one or more nodes fail. Data is divided into blocks in a cluster and replicated across numerous nodes. In this case, if one node goes down, the user can still access the data on other machines. HDFS maintains its replication process periodically.
  
  
**Terms related to HDFS:**  

- _**HeartBeat**_ : It is the signal that datanode continuously sends to namenode. If namenode doesn’t receive heartbeat from a datanode then it will consider it dead.
- _**Balancing**_ : If a datanode is crashed the blocks present on it will be gone too and the blocks will be _under-replicated_ compared to the remaining blocks. Here master node(namenode) will give a signal to data_nodes containing replicas of those lost blocks to replicate so that overall distribution of blocks is balanced.
- _**Replication:**_: It is done by datanode.

  
**Note:** No two replicas of the same block are present on the same datanode.   
  
**Features:**  

- Distributed data storage.
- Blocks reduce seek time.
- The data is highly available as the same block is present at multiple datanodes.
- Even if multiple datanodes are down we can still do our work, thus making it highly reliable.
- High fault tolerance.

  
**Limitations:** Though HDFS provide many features there are some areas where it doesn’t work well. 

- **Low latency data access**: Applications that require low-latency access to data i.e in the range of milliseconds will not work well with HDFS, because HDFS is designed keeping in mind that we need high-throughput of data even at the cost of latency.
- **Small file problem**: Having lots of small files will result in lots of seeks and lots of movement from one datanode to another datanode to retrieve each small file, this whole process is a very inefficient data access pattern.


#### Map Reduce 

#### YARN 

#### Nodes 
##### Client Node 

##### Name Node 
	- FS Image 
	- Edit Log 

	Seconday Node 

#### Slave Node 


#### Rack 



#### ZooKeeper
![[Pasted image 20240224155834.png]]


#### Benefits of HDFS

The benefits of the Hadoop Distributed File System are as follows:

1) The Hadoop Distributed File System is designed for big data, not only for storing big data but also for facilitating the processing of big data.

2) HDFS is cost-effective because it can be run on cheap hardware and does not require a powerful machine.

3) HDFS has high fault tolerance since if a machine within a cluster fails, a replica of the data may be available from a different node through replication.

4) Hadoop is famous for its rack awareness to avoid data loss, which results in increased latency.

5) HDFS is scalable, and it includes vertical and horizontal scalability mechanisms so you can adjust the resources according to the size of your file system.

6) Streaming reads are made possible through HDFS.

> [!NOTE]
> jknaContent


